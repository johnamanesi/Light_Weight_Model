{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "96hKsIGKgap_",
        "outputId": "426d5a11-2252-4b4e-c2de-38382ac1eb8d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a6ba705a-b3b5-43c0-8865-e2497bec43f0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a6ba705a-b3b5-43c0-8865-e2497bec43f0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving archive (2).zip to archive (2).zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import time\n",
        "import zipfile\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import torchvision\n",
        "\n",
        "# Define your model\n",
        "class IRShuffleUnit(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(IRShuffleUnit, self).__init__()\n",
        "        self.stride = stride\n",
        "        mid_channels = in_channels * 2\n",
        "        self.pwconv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.dwconv = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=stride, padding=1, groups=mid_channels, bias=False)\n",
        "        self.pwconv2 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(mid_channels)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pwconv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.prelu(out)\n",
        "        out = self.dwconv(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.prelu(out)\n",
        "        out = self.pwconv2(out)\n",
        "        out = self.bn3(out)\n",
        "        return out\n",
        "\n",
        "class LightweightCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(LightweightCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.irshuffle_blocks = nn.Sequential(\n",
        "            IRShuffleUnit(32, 48, 2),\n",
        "            IRShuffleUnit(48, 48, 1),\n",
        "            IRShuffleUnit(48, 96, 2),\n",
        "            IRShuffleUnit(96, 96, 1),\n",
        "            IRShuffleUnit(96, 96, 1),\n",
        "            IRShuffleUnit(96, 96, 1),\n",
        "            IRShuffleUnit(96, 192, 2),\n",
        "            IRShuffleUnit(192, 192, 1)\n",
        "        )\n",
        "        self.pwconv1 = nn.Conv2d(192, 512, kernel_size=1, bias=False)\n",
        "        self.dwconv = nn.Conv2d(512, 512, kernel_size=3, padding=1, groups=512, bias=False)\n",
        "        self.pwconv2 = nn.Conv2d(512, 128, kernel_size=1, bias=False)\n",
        "        self.fc = nn.Linear(128 * 7 * 6, num_classes)  # Modify the input size accordingly\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.irshuffle_blocks(x)\n",
        "        x = F.relu(self.pwconv1(x))\n",
        "        x = F.relu(self.dwconv(x))\n",
        "        x = self.pwconv2(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output\n",
        "        x = self.fc(x)  # Apply the fully connected layer\n",
        "        return x\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.img_paths = []\n",
        "        self.labels = []\n",
        "        self.class_to_idx = {}\n",
        "\n",
        "        for root, _, files in os.walk(root_dir):\n",
        "            class_name = os.path.basename(root)\n",
        "            if class_name not in self.class_to_idx:\n",
        "                self.class_to_idx[class_name] = len(self.class_to_idx)\n",
        "            for file in files:\n",
        "                if file.endswith(('png', 'jpg', 'jpeg')):\n",
        "                    self.img_paths.append(os.path.join(root, file))\n",
        "                    self.labels.append(self.class_to_idx[class_name])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        return img, label\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((112, 96)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Upload and extract dataset\n",
        "uploaded = files.upload()\n",
        "zip_path = list(uploaded.keys())[0]\n",
        "extracted_path = '/content/dataset'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_path)\n",
        "\n",
        "dataset = ImageDataset(extracted_path, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "# Instantiate the model with the correct number of classes\n",
        "model = LightweightCNN(num_classes=len(dataset.class_to_idx))\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        if labels.dim() != 1:\n",
        "            labels = labels.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    print(f'Epoch {epoch}/{num_epochs - 1}, Loss: {epoch_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghSYDWiElaPo",
        "outputId": "d7bf573d-efbd-47f3-ca5c-1e73c5f0c8df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/24, Loss: 8.1733\n",
            "Epoch 1/24, Loss: 7.3125\n",
            "Epoch 2/24, Loss: 6.1389\n",
            "Epoch 3/24, Loss: 4.5568\n",
            "Epoch 4/24, Loss: 2.9557\n",
            "Epoch 5/24, Loss: 1.8846\n",
            "Epoch 6/24, Loss: 1.2749\n",
            "Epoch 7/24, Loss: 0.8776\n",
            "Epoch 8/24, Loss: 0.6535\n",
            "Epoch 9/24, Loss: 0.5124\n",
            "Epoch 10/24, Loss: 0.4158\n",
            "Epoch 11/24, Loss: 0.3754\n",
            "Epoch 12/24, Loss: 0.3078\n",
            "Epoch 13/24, Loss: 0.3012\n",
            "Epoch 14/24, Loss: 0.2585\n",
            "Epoch 15/24, Loss: 0.2168\n",
            "Epoch 16/24, Loss: 0.2312\n",
            "Epoch 17/24, Loss: 0.2411\n",
            "Epoch 18/24, Loss: 0.1954\n",
            "Epoch 19/24, Loss: 0.2041\n",
            "Epoch 20/24, Loss: 0.1905\n",
            "Epoch 21/24, Loss: 0.1718\n",
            "Epoch 22/24, Loss: 0.1597\n",
            "Epoch 23/24, Loss: 0.1524\n",
            "Epoch 24/24, Loss: 0.1548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'lightweight_cnn.pth')"
      ],
      "metadata": {
        "id": "rCyPcOnQrowu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate the model\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    total_time = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            start_time = time.time()\n",
        "            outputs = model(inputs)\n",
        "            end_time = time.time()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            total_time += end_time - start_time\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    avg_inference_time = total_time / len(all_labels)\n",
        "\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1 Score: {f1:.4f}')\n",
        "    print(f'Average Inference Time per Image: {avg_inference_time:.6f} seconds')\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GymfrNmocYe",
        "outputId": "01b85cb5-4457-44bc-82f8-40337143f0fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9636\n",
            "Precision: 0.9684\n",
            "Recall: 0.9636\n",
            "F1 Score: 0.9630\n",
            "Average Inference Time per Image: 0.012824 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, log_loss, matthews_corrcoef, cohen_kappa_score, classification_report\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "104A87-Cr1JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate log loss\n",
        "def calculate_log_loss(model, dataloader):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.softmax(outputs, dim=1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    loss = log_loss(all_labels, all_preds)\n",
        "    print(f'Log Loss: {loss:.4f}')"
      ],
      "metadata": {
        "id": "ugaw5VgAtIdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "num_classes = len(dataset.class_to_idx)  # Ensure this matches your dataset\n",
        "model = LightweightCNN(num_classes)\n",
        "model.load_state_dict(torch.load('lightweight_cnn.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqcf5cgxuuBA",
        "outputId": "9607b291-8615-491a-9f46-f4c2ba5c0397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LightweightCNN(\n",
              "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (irshuffle_blocks): Sequential(\n",
              "    (0): IRShuffleUnit(\n",
              "      (pwconv1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (dwconv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
              "      (pwconv2): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=1)\n",
              "    )\n",
              "    (1): IRShuffleUnit(\n",
              "      (pwconv1): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
              "      (pwconv2): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=1)\n",
              "    )\n",
              "    (2): IRShuffleUnit(\n",
              "      (pwconv1): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "      (pwconv2): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=1)\n",
              "    )\n",
              "    (3): IRShuffleUnit(\n",
              "      (pwconv1): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "      (pwconv2): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=1)\n",
              "    )\n",
              "    (4): IRShuffleUnit(\n",
              "      (pwconv1): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "      (pwconv2): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=1)\n",
              "    )\n",
              "    (5): IRShuffleUnit(\n",
              "      (pwconv1): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "      (pwconv2): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=1)\n",
              "    )\n",
              "    (6): IRShuffleUnit(\n",
              "      (pwconv1): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
              "      (pwconv2): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=1)\n",
              "    )\n",
              "    (7): IRShuffleUnit(\n",
              "      (pwconv1): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "      (pwconv2): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=1)\n",
              "    )\n",
              "  )\n",
              "  (pwconv1): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
              "  (pwconv2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (fc): Linear(in_features=5376, out_features=5751, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to measure latency\n",
        "def measure_latency(model, image_path, transform, num_iterations=100):\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img = transform(img).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Warm up the model\n",
        "    for _ in range(10):\n",
        "        _ = model(img)\n",
        "\n",
        "    # Measure latency\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_iterations):\n",
        "        _ = model(img)\n",
        "    end_time = time.time()\n",
        "\n",
        "    avg_latency = (end_time - start_time) / num_iterations\n",
        "    return avg_latency\n",
        "\n",
        "# Upload a file\n",
        "#uploaded = files.upload()\n",
        "\n",
        "# Check if the uploaded file is an image or a zip file\n",
        "uploaded_file = list(uploaded.keys())[0]\n",
        "if uploaded_file.endswith(('.jpg', '.jpeg', '.png')):\n",
        "    image_path = uploaded_file\n",
        "elif uploaded_file.endswith('.zip'):\n",
        "    with zipfile.ZipFile(uploaded_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content/')\n",
        "        # Find the first image file in the extracted content\n",
        "        for root, _, files in os.walk('/content/'):\n",
        "            for file in files:\n",
        "                if file.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    image_path = os.path.join(root, file)\n",
        "                    break\n",
        "\n",
        "# Measure latency for the image\n",
        "avg_latency = measure_latency(model, image_path, transform)\n",
        "print(f'Average latency per image: {avg_latency:.6f} seconds')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_J-zXNpvYTU",
        "outputId": "d7dd5605-9f26-4f20-bc19-204742938b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average latency per image: 0.026631 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "def plot_roc_curve(model, dataloader, num_classes):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            outputs = model(inputs)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    all_labels = label_binarize(all_labels, classes=range(num_classes))\n",
        "    fpr = {}\n",
        "    tpr = {}\n",
        "    roc_auc = {}\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(all_labels[:, i], [prob[i] for prob in all_probs])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for i in range(num_classes):\n",
        "        plt.plot(fpr[i], tpr[i], label=f'Class {i} (area = {roc_auc[i]:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "# Plot ROC curve for validation set\n",
        "plot_roc_curve(model, val_dataloader, len(dataset.class_to_idx))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "mNl-2waAzn1G",
        "outputId": "4e6fb9c5-3b42-4a7a-cf70-aa952f388652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'val_dataloader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-0ee9c19d4fb0>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Plot ROC curve for validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mplot_roc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'val_dataloader' is not defined"
          ]
        }
      ]
    }
  ]
}